{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://www.dropbox.com/scl/fo/5w0a14icfv4o7t0azrqda/AHECd04T6OAUwcvxwiZXw-4/data/tmall?rlkey=qhx7csgahlcbuppjx4ewa3l0o&subfolder_nav_tracking=1&st=kqkod5je&dl=1\"\n",
    "output_path = \"tmall.zip\"\n",
    "\n",
    "print(\"Downloading...\")\n",
    "urllib.request.urlretrieve(url, output_path)\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zip\n",
    "import os\n",
    "\n",
    "with zip.ZipFile(output_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"tmall_data\")\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S.Soudeep\\Documents\\Contrastive_Spiking_Graph_NeuralNetwork\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dropout_edge\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import random\n",
    "\n",
    "# ============================================\n",
    "# Set Seeds for Reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Model Components (from previous artifact)\n",
    "# ============================================\n",
    "\n",
    "class LIFNeuron(nn.Module):\n",
    "    def __init__(self, decay=0.95, threshold=1.0):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def forward(self, x, membrane=None):\n",
    "        if membrane is None:\n",
    "            membrane = torch.zeros_like(x)\n",
    "        \n",
    "        membrane = self.decay * membrane + x\n",
    "        spike = torch.sigmoid(5 * (membrane - self.threshold))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reset_mask = (spike > 0.5).float()\n",
    "            membrane = membrane - reset_mask * self.threshold\n",
    "            \n",
    "        return spike, membrane\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, time_steps):\n",
    "        super().__init__()\n",
    "        self.attention_weights = nn.Parameter(torch.randn(time_steps))\n",
    "        \n",
    "    def forward(self, spikes_list):\n",
    "        # Memory efficient implementation:\n",
    "        # Avoids creating a large stacked tensor (T, N, C)\n",
    "        # and avoids creating a large weighted tensor (T, N, C)\n",
    "        alpha = F.softmax(self.attention_weights, dim=0)\n",
    "        output = 0\n",
    "        for t, spike in enumerate(spikes_list):\n",
    "            output = output + alpha[t] * spike\n",
    "        return output\n",
    "\n",
    "\n",
    "class SpikingGCN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_steps):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "        self.lif = LIFNeuron(decay=0.95, threshold=1.0)\n",
    "        self.time_steps = time_steps\n",
    "        self.temp_attn = TemporalAttention(time_steps)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)\n",
    "        \n",
    "        spikes = []\n",
    "        membrane = None\n",
    "        \n",
    "        for t in range(self.time_steps):\n",
    "            spike, membrane = self.lif(x, membrane)\n",
    "            spikes.append(spike)\n",
    "        \n",
    "        # Pass list of spikes to attention instead of stacking\n",
    "        output = self.temp_attn(spikes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class ImprovedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, z1, z2, batch_size=None):\n",
    "        z1 = F.normalize(z1, dim=1)\n",
    "        z2 = F.normalize(z2, dim=1)\n",
    "        \n",
    "        N = z1.size(0)\n",
    "        \n",
    "        if batch_size is not None and batch_size < N:\n",
    "            indices = torch.randperm(N)[:batch_size]\n",
    "            z1 = z1[indices]\n",
    "            z2 = z2[indices]\n",
    "            N = batch_size\n",
    "        \n",
    "        representations = torch.cat([z1, z2], dim=0)\n",
    "        similarity_matrix = torch.mm(representations, representations.T)\n",
    "        similarity_matrix = similarity_matrix / self.temperature\n",
    "        \n",
    "        mask = torch.eye(2 * N, dtype=torch.bool, device=z1.device)\n",
    "        similarity_matrix.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        labels = torch.cat([\n",
    "            torch.arange(N, 2*N, device=z1.device),\n",
    "            torch.arange(N, device=z1.device)\n",
    "        ])\n",
    "        \n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DyC_DGNN(nn.Module):\n",
    "    def __init__(self, num_nodes, in_channels, hidden_channels, out_channels, time_steps=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Learnable node embeddings - CRITICAL for datasets with random/no features\n",
    "        self.node_emb = nn.Embedding(num_nodes, in_channels)\n",
    "        nn.init.xavier_uniform_(self.node_emb.weight)\n",
    "        \n",
    "        self.spike_gcn1 = SpikingGCN(in_channels, hidden_channels, time_steps)\n",
    "        self.spike_gcn2 = SpikingGCN(hidden_channels, hidden_channels, time_steps)\n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_channels, out_channels)\n",
    "        self.contrastive_loss_fn = ImprovedContrastiveLoss(temperature=0.5)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "    def encode(self, x, edge_index):\n",
    "        x = F.relu(self.spike_gcn1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        embeddings = self.spike_gcn2(x, edge_index)\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Note: x should be passed from outside (usually self.node_emb.weight)\n",
    "        embeddings = self.encode(x, edge_index)\n",
    "        logits = self.classifier(embeddings)\n",
    "        projections = self.projector(embeddings)\n",
    "        return logits, projections, embeddings\n",
    "    \n",
    "    def augment_graph(self, x, edge_index, drop_edge_p=0.3, drop_feat_p=0.2):\n",
    "        edge_index_aug, _ = dropout_edge(edge_index, p=drop_edge_p, \n",
    "                                         force_undirected=True)\n",
    "        \n",
    "        x_aug = x.clone()\n",
    "        mask = torch.rand(x.size(), device=x.device) > drop_feat_p\n",
    "        x_aug = x_aug * mask.float()\n",
    "        \n",
    "        return x_aug, edge_index_aug\n",
    "    \n",
    "    def contrastive_loss(self, x, edge_index, batch_size=512):\n",
    "        x_aug1, edge_index_aug1 = self.augment_graph(x, edge_index)\n",
    "        x_aug2, edge_index_aug2 = self.augment_graph(x, edge_index)\n",
    "        \n",
    "        _, proj1, _ = self.forward(x_aug1, edge_index_aug1)\n",
    "        _, proj2, _ = self.forward(x_aug2, edge_index_aug2)\n",
    "        \n",
    "        if x.size(0) > batch_size:\n",
    "            indices = torch.randperm(x.size(0))[:batch_size]\n",
    "            proj1 = proj1[indices]\n",
    "            proj2 = proj2[indices]\n",
    "        \n",
    "        loss = self.contrastive_loss_fn(proj1, proj2)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def full_loss(self, logits, y, x, edge_index, mask, use_contrastive=True, class_weight=None):\n",
    "        # Apply class weights to handle imbalance\n",
    "        cls_loss = F.cross_entropy(logits[mask], y[mask], weight=class_weight)\n",
    "        \n",
    "        if use_contrastive and self.training:\n",
    "            cont_loss = self.contrastive_loss(x, edge_index, batch_size=512)\n",
    "            alpha = torch.sigmoid(self.alpha)\n",
    "            total_loss = alpha * cls_loss + (1 - alpha) * cont_loss\n",
    "            return total_loss, cls_loss.item(), cont_loss.item()\n",
    "        else:\n",
    "            return cls_loss, cls_loss.item(), 0.0\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FIXED: Data Loading for Tmall\n",
    "# ============================================\n",
    "\n",
    "def load_tmall_data(edges_file, nodes_file):\n",
    "    \"\"\"\n",
    "    Fixed data loading with proper handling of unlabeled nodes\n",
    "    \"\"\"\n",
    "    current_idx = 0\n",
    "    node_id_to_idx = {}\n",
    "    labels = []\n",
    "    \n",
    "    # Load labeled nodes\n",
    "    with open(nodes_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            node_id, label = map(int, line.strip().split())\n",
    "            if node_id not in node_id_to_idx:\n",
    "                node_id_to_idx[node_id] = current_idx\n",
    "                labels.append(label)\n",
    "                current_idx += 1\n",
    "    \n",
    "    # Load edges and add unlabeled nodes\n",
    "    edges = []\n",
    "    with open(edges_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            src, dst = map(int, parts[:2])\n",
    "            \n",
    "            for node in [src, dst]:\n",
    "                if node not in node_id_to_idx:\n",
    "                    node_id_to_idx[node] = current_idx\n",
    "                    labels.append(-1)  # Unlabeled\n",
    "                    current_idx += 1\n",
    "            \n",
    "            edges.append([node_id_to_idx[src], node_id_to_idx[dst]])\n",
    "    \n",
    "    # Create tensors\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    # We still create x but it will be ignored by the model in favor of embeddings\n",
    "    x = torch.randn(len(node_id_to_idx), 16)  \n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    \n",
    "    # Validation\n",
    "    assert data.edge_index.max() < data.num_nodes, \"Invalid edge indices!\"\n",
    "    \n",
    "    print(f\"Loaded Tmall dataset:\")\n",
    "    print(f\"  Nodes: {data.num_nodes}\")\n",
    "    print(f\"  Edges: {data.num_edges}\")\n",
    "    print(f\"  Labeled nodes: {(y != -1).sum().item()}\")\n",
    "    print(f\"  Features: {data.x.shape}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_split(data, train_ratio, run_seed):\n",
    "    \"\"\"\n",
    "    Fixed stratified split that handles unlabeled nodes\n",
    "    \"\"\"\n",
    "    # Get labeled nodes only\n",
    "    labeled_mask = data.y != -1\n",
    "    labeled_indices = torch.where(labeled_mask)[0].numpy()\n",
    "    labeled_y = data.y[labeled_mask].numpy()\n",
    "    \n",
    "    # Stratified split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1-train_ratio, \n",
    "                                 random_state=run_seed)\n",
    "    train_idx_labeled, temp_idx_labeled = next(sss.split(labeled_indices, labeled_y))\n",
    "    \n",
    "    # Second split for val/test\n",
    "    temp_indices = labeled_indices[temp_idx_labeled]\n",
    "    temp_y = labeled_y[temp_idx_labeled]\n",
    "    \n",
    "    sss_temp = StratifiedShuffleSplit(n_splits=1, test_size=0.5, \n",
    "                                      random_state=run_seed)\n",
    "    val_idx_temp, test_idx_temp = next(sss_temp.split(temp_indices, temp_y))\n",
    "    \n",
    "    # Map back to original indices\n",
    "    train_idx = labeled_indices[train_idx_labeled]\n",
    "    val_idx = temp_indices[val_idx_temp]\n",
    "    test_idx = temp_indices[test_idx_temp]\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    print(f\"Split created:\")\n",
    "    print(f\"  Train: {train_mask.sum().item()}\")\n",
    "    print(f\"  Val: {val_mask.sum().item()}\")\n",
    "    print(f\"  Test: {test_mask.sum().item()}\")\n",
    "    \n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def train_model(data, train_mask, val_mask, test_mask, \n",
    "                hidden_channels=64, time_steps=5, epochs=100, lr=0.001):\n",
    "    \"\"\"\n",
    "    Fixed training procedure with Class Weights and Learnable Embeddings\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    num_classes = int(data.y[data.y != -1].max()) + 1\n",
    "    \n",
    "    # Calculate class weights\n",
    "    train_y = data.y[train_mask]\n",
    "    class_counts = torch.bincount(train_y)\n",
    "    # Handle potential missing classes in small splits\n",
    "    if len(class_counts) < num_classes:\n",
    "        class_counts = torch.cat([class_counts, torch.zeros(num_classes - len(class_counts), dtype=torch.long)])\n",
    "    \n",
    "    # Inverse frequency weights\n",
    "    class_weights = 1.0 / (class_counts.float() + 1e-5)\n",
    "    class_weights = class_weights / class_weights.sum() * num_classes  # Normalize\n",
    "    \n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    # Use embedding size of 32\n",
    "    embedding_dim = 32\n",
    "    \n",
    "    model = DyC_DGNN(\n",
    "        num_nodes=data.num_nodes,\n",
    "        in_channels=embedding_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        out_channels=num_classes,\n",
    "        time_steps=time_steps\n",
    "    )\n",
    "    \n",
    "    # FORCE CPU usage to avoid CUDA version mismatch\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    class_weights = class_weights.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use learnable embeddings\n",
    "        x_input = model.node_emb.weight\n",
    "        \n",
    "        logits, proj, _ = model(x_input, data.edge_index)\n",
    "        \n",
    "        total_loss, cls_loss, cont_loss = model.full_loss(\n",
    "            logits, data.y, x_input, data.edge_index, \n",
    "            train_mask, use_contrastive=True, class_weight=class_weights\n",
    "        )\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_input = model.node_emb.weight\n",
    "                logits, _, _ = model(x_input, data.edge_index)\n",
    "                pred = logits.argmax(dim=1)\n",
    "                \n",
    "                val_f1 = f1_score(\n",
    "                    data.y[val_mask].cpu(),\n",
    "                    pred[val_mask].cpu(),\n",
    "                    average='macro',\n",
    "                    zero_division=0\n",
    "                )\n",
    "                \n",
    "                test_f1 = f1_score(\n",
    "                    data.y[test_mask].cpu(),\n",
    "                    pred[test_mask].cpu(),\n",
    "                    average='macro',\n",
    "                    zero_division=0\n",
    "                )\n",
    "                \n",
    "                print(f\"Epoch {epoch:3d} | \"\n",
    "                      f\"Loss: {total_loss:.4f} \"\n",
    "                      f\"(Cls: {cls_loss:.4f}, Cont: {cont_loss:.4f}) | \"\n",
    "                      f\"Val F1: {val_f1:.4f} | Test F1: {test_f1:.4f}\")\n",
    "                \n",
    "                if val_f1 > best_val_f1:\n",
    "                    best_val_f1 = val_f1\n",
    "                    best_model_state = {k: v.cpu().clone() \n",
    "                                       for k, v in model.state_dict().items()}\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "    \n",
    "    # Load best model and evaluate\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_input = model.node_emb.weight\n",
    "        logits, _, _ = model(x_input, data.edge_index)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        \n",
    "        test_f1_macro = f1_score(\n",
    "            data.y[test_mask].cpu(),\n",
    "            pred[test_mask].cpu(),\n",
    "            average='macro',\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        test_f1_micro = f1_score(\n",
    "            data.y[test_mask].cpu(),\n",
    "            pred[test_mask].cpu(),\n",
    "            average='micro',\n",
    "            zero_division=0\n",
    "        )\n",
    "    \n",
    "    return test_f1_macro, test_f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(data, training_ratios=[0.4, 0.6, 0.8], num_runs=5):\n",
    "    \"\"\"\n",
    "    Run complete experiments with multiple runs and ratios\n",
    "    \"\"\"\n",
    "    results_macro = {ratio: [] for ratio in training_ratios}\n",
    "    results_micro = {ratio: [] for ratio in training_ratios}\n",
    "    \n",
    "    for ratio in training_ratios:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Ratio: {ratio*100}%\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            print(f\"\\n--- Run {run+1}/{num_runs} ---\")\n",
    "            set_seed(42 + run)\n",
    "            \n",
    "            # Create split\n",
    "            train_mask, val_mask, test_mask = create_stratified_split(\n",
    "                data, ratio, run_seed=42+run\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            # Reduced hidden_channels and time_steps to prevent OOM\n",
    "            test_f1_macro, test_f1_micro = train_model(\n",
    "                data, train_mask, val_mask, test_mask,\n",
    "                hidden_channels=32,  # Reduced from 64\n",
    "                time_steps=3,        # Reduced from 5\n",
    "                epochs=40,\n",
    "                lr=0.001\n",
    "            )\n",
    "            \n",
    "            results_macro[ratio].append(test_f1_macro)\n",
    "            results_micro[ratio].append(test_f1_micro)\n",
    "            \n",
    "            print(f\"Run {run+1} Results:\")\n",
    "            print(f\"  Macro-F1: {test_f1_macro:.4f}\")\n",
    "            print(f\"  Micro-F1: {test_f1_micro:.4f}\")\n",
    "    \n",
    "    # Print final results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\nMacro-F1 (Mean ± Std):\")\n",
    "    for ratio in training_ratios:\n",
    "        scores = results_macro[ratio]\n",
    "        mean = np.mean(scores) * 100\n",
    "        std = np.std(scores) * 100\n",
    "        print(f\"  {ratio*100:.0f}% Training: {mean:.2f} ± {std:.2f}\")\n",
    "    \n",
    "    print(\"\\nMicro-F1 (Mean ± Std):\")\n",
    "    for ratio in training_ratios:\n",
    "        scores = results_micro[ratio]\n",
    "        mean = np.mean(scores) * 100\n",
    "        std = np.std(scores) * 100\n",
    "        print(f\"  {ratio*100:.0f}% Training: {mean:.2f} ± {std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    data = load_tmall_data(\n",
    "        edges_file=\"tmall_data/tmall.txt\",\n",
    "        nodes_file=\"tmall_data/node2label.txt\"\n",
    "    )\n",
    "    \n",
    "    # Run experiments\n",
    "    run_experiments(data, training_ratios=[0.4, 0.6, 0.8], num_runs=5)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Contrastive_Spiking_Graph_NeuralNetwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
